{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-Show-and-Tell.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "onfAMZQdToCR",
        "colab": {},
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding,BatchNormalization, Dropout, TimeDistributed, Dense,Concatenate, RepeatVector, Activation, Input, add\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from keras import backend \n",
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_jqquK3T19oD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Load all the captions and format them for the LSTM layers, divided in training and test set\n",
        "\n",
        "# Make caption dictionary whose keys are image file name and values are image caption\n",
        "token_dir=\"Flickr8k.token.txt\"\n",
        "\n",
        "image_captions = open(token_dir).read().split('\\n')\n",
        "caption = {}    \n",
        "for i in range(len(image_captions)-1):\n",
        "    id_capt = image_captions[i].split(\"\\t\")\n",
        "    id_capt[0] = id_capt[0][:len(id_capt[0])-2] # get rid of the #0,#1,#2,#3,#4 from the tokens file\n",
        "    if id_capt[0] in caption:\n",
        "        caption[id_capt[0]].append(id_capt[1])\n",
        "    else:\n",
        "        caption[id_capt[0]] = [id_capt[1]]\n",
        "\n",
        "\n",
        "# Make two files named \"trainImages.txt\" and \"testImages.txt\" that will have start and end token at the start and end of each caption respectively.\n",
        "\n",
        "train_imgs_id = open(\"Flickr_8k.trainImages.txt\").read().split('\\n')[:-1]\n",
        "\n",
        "\n",
        "train_imgs_captions = open(\"trainImages.txt\",'w')\n",
        "for img_id in train_imgs_id:\n",
        "    for captions in caption[img_id]:\n",
        "        desc = \"<start> \"+captions+\" <end>\"\n",
        "        train_imgs_captions.write(img_id+\"\\t\"+desc+\"\\n\")\n",
        "        train_imgs_captions.flush()\n",
        "train_imgs_captions.close()\n",
        "\n",
        "test_imgs_id = open(\"Flickr_8k.testImages.txt\").read().split('\\n')[:-1]\n",
        "\n",
        "test_imgs_captions = open(\"testImages.txt\",'w')\n",
        "for img_id in test_imgs_id:\n",
        "    for captions in caption[img_id]:\n",
        "        desc = \"<start> \"+captions+\" <end>\"\n",
        "        test_imgs_captions.write(img_id+\"\\t\"+desc+\"\\n\")\n",
        "        test_imgs_captions.flush()\n",
        "test_imgs_captions.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CpeU_sz5UGeA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load inception"
      ]
    },
    {
      "metadata": {
        "id": "DCzHQcj7T8Is",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dde3a5d1-19ac-47cc-f39f-7f645a7e7904"
      },
      "cell_type": "code",
      "source": [
        "#Load InceptionV3\n",
        "model = InceptionV3(weights='imagenet')\n",
        "\n",
        "\n",
        "new_input = model.input\n",
        "new_output = model.layers[-2].output\n",
        "\n",
        "model_new = Model(new_input, new_output)\n",
        "\n",
        "#Pre-process the images_pathto get the predicion tf style from inception\n",
        "\n",
        "# Scale the pixels between -1 and 1, sample wise. (from keras preprocessing)\n",
        "def preprocess_input(x):\n",
        "\tx /= 127.5\n",
        "\tx -= 1.\n",
        "\treturn x\n",
        "\n",
        "# Convert all the images_pathinto a numpy array of 3 dimension plus one.\n",
        "def preprocess(image_path):\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Get the prediction from inceptionV3 for the given image\n",
        "def encode(image):\n",
        "    image = preprocess(image)\n",
        "    temp_enc = model_new.predict(image)\n",
        "    temp_enc = np.reshape(temp_enc, temp_enc.shape[1])\n",
        "    return temp_enc\n",
        "\n",
        "\n",
        "\n",
        "images_path= \"/content/drive/My Drive/Colab Notebooks/Flicker8k_Dataset/\"\n",
        "\n",
        "train_imgs_id = open(\"Flickr_8k.trainImages.txt\").read().split('\\n')[:-1]\n",
        "test_imgs_id = open(\"Flickr_8k.testImages.txt\").read().split('\\n')[:-1]\n",
        "\n",
        "# Encode/get prediction for both training and test images\n",
        "\n",
        "encoding_train = {}\n",
        "# with tqdm we'll get a nice progress bar for the loops\n",
        "#for img in tqdm(train_imgs_id): \n",
        "#    path = images_path+str(img)\n",
        "#    encoding_train[img] = encode(path)\n",
        "\n",
        "\n",
        "#with open(\"encoded_train_images_inceptionV3.p\", \"wb\") as encoded_pickle: \n",
        "#    pickle.dump(encoding_train, encoded_pickle)  \n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96116736/96112376 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e26sYIuaKtTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoding_train = pickle.load(open('encoded_train_images_inceptionV3.p', 'rb'))\n",
        "\n",
        "encoding_test = {}\n",
        "#for img in tqdm(test_imgs_id):\n",
        "#    path = images_path+str(img)\n",
        "#    encoding_test[img] = encode(path)\n",
        "\n",
        "#with open(\"encoded_test_images_inceptionV3.p\", \"wb\") as encoded_pickle:\n",
        "#    pickle.dump(encoding_test, encoded_pickle)\n",
        "\n",
        "\n",
        "encoding_test = pickle.load(open('encoded_test_images_inceptionV3.p', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2D-1eFy3QcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv(\"trainImages.txt\", delimiter='\\t')\n",
        "captionz = []\n",
        "img_id = []\n",
        "dataframe = dataframe.sample(frac=1)\n",
        "iter = dataframe.iterrows()\n",
        "\n",
        "for i in range(len(dataframe)):\n",
        "    nextiter = next(iter)\n",
        "    captionz.append(nextiter[1][1])\n",
        "    img_id.append(nextiter[1][0])\n",
        "\n",
        "no_samples=0\n",
        "tokens = []\n",
        "tokens = [i.split() for i in captionz]\n",
        "for caption in captionz:\n",
        "    no_samples+=len(caption.split())-1\n",
        "\n",
        "\n",
        "# Load from file it's much faster then recreating the vocab each time.\n",
        "vocab= [] \n",
        "vocab = list(set(vocab))\n",
        "vocab= pickle.load(open('vocab.p', 'rb'))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_idx = {val:index for index, val in enumerate(vocab)}\n",
        "idx_word = {index:val for index, val in enumerate(vocab)}\n",
        "\n",
        "\n",
        "caption_length = [len(caption.split()) for caption in captionz]\n",
        "max_length = max(caption_length)\n",
        "\n",
        "\n",
        "# Create batches for training\n",
        "def data_process(batch_size):\n",
        "    partial_captions = []\n",
        "    next_words = []\n",
        "    images= []\n",
        "    total_count = 0\n",
        "    while 1:\n",
        "    \n",
        "        for image_counter, caption in enumerate(captionz):\n",
        "            current_image = encoding_train[img_id[image_counter]]\n",
        "    \n",
        "            for i in range(len(caption.split())-1):\n",
        "                total_count+=1\n",
        "                partial = [word_idx[txt] for txt in caption.split()[:i+1]]\n",
        "                partial_captions.append(partial)\n",
        "                next = np.zeros(vocab_size)\n",
        "                #OHE the captions\n",
        "                next[word_idx[caption.split()[i+1]]] = 1\n",
        "                next_words.append(next)\n",
        "                images.append(current_image)\n",
        "\n",
        "                if total_count>=batch_size:\n",
        "                    next_words = np.asarray(next_words)\n",
        "                    images= np.asarray(images)\n",
        "                    partial_captions = sequence.pad_sequences(partial_captions, maxlen=max_length, padding='post')\n",
        "                    total_count = 0\n",
        "                \n",
        "                    yield [[images, partial_captions], next_words]\n",
        "                    partial_captions = []\n",
        "                    next_words = []\n",
        "                    images= []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "woaCyLAB3kfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#------ Now we build the encoder-decoder model ----------------------------------------------#\n",
        "#from keras.layers import Merge\n",
        "\n",
        "EMBEDDING_DIM = 300 \n",
        "\n",
        "# Model\n",
        "\n",
        "image_inputs = Input(shape=(2048,))\n",
        "image_model = Dropout(0.5)(image_inputs)\n",
        "image_model_D = Dense(256, activation='relu')(image_model)\n",
        "\n",
        "lang_inputs = Input(shape=(max_length,))\n",
        "lang_model = Embedding(vocab_size, 300, mask_zero=True)(lang_inputs)\n",
        "lang_model_d = Dropout(0.5)(lang_model)\n",
        "lang_model_s = LSTM(256)(lang_model_d)\n",
        "\n",
        "decoder = add([image_model_D, lang_model_s])\n",
        "decoder_d = Dense(256, activation='relu')(decoder)\n",
        "\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder_d)\n",
        "final_model = Model(inputs=[image_inputs, lang_inputs], outputs=outputs)\n",
        "print (\"Model created!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "amIsEa4l3nkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "64891721-d9b4-4878-b7ab-0dffd0ecbf16"
      },
      "cell_type": "code",
      "source": [
        "epoch = 5\n",
        "batch_size = 512\n",
        "final_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "generator = data_process(batch_size=batch_size)\n",
        "final_model.fit_generator(generator, steps_per_epoch=no_samples/batch_size, epochs=epoch, verbose=1, callbacks=None)\n",
        "\n",
        "# save the entire model for later use\n",
        "final_model.save('showandtell.h5')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "749/748 [==============================] - 154s 205ms/step - loss: 3.0515 - acc: 0.4032\n",
            "Epoch 2/5\n",
            "749/748 [==============================] - 152s 203ms/step - loss: 2.8168 - acc: 0.4195\n",
            "Epoch 3/5\n",
            "749/748 [==============================] - 152s 203ms/step - loss: 2.6578 - acc: 0.4310\n",
            "Epoch 4/5\n",
            "749/748 [==============================] - 152s 202ms/step - loss: 2.5367 - acc: 0.4408\n",
            "Epoch 5/5\n",
            "749/748 [==============================] - 151s 202ms/step - loss: 2.4408 - acc: 0.4484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GEPBLa7u3w1P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Standard predictions\n",
        "def predict_captions(image_file):\n",
        "    start_word = [\"<start>\"]\n",
        "    while 1:\n",
        "        now_caps = [word_idx[i] for i in start_word]\n",
        "        now_caps = sequence.pad_sequences([now_caps], maxlen=max_length, padding='post')\n",
        "        e = encoding_test[image_file]\n",
        "        preds = final_model.predict([np.array([e]), np.array(now_caps)])\n",
        "        word_pred = idx_word[np.argmax(preds[0])]\n",
        "        start_word.append(word_pred)\n",
        "        \n",
        "        if word_pred == \"<end>\" or len(start_word) > max_length: \n",
        "    #keep on predicting next word unitil word predicted is <end> or caption lenghts is greater than max_lenght(40)\n",
        "            break\n",
        "            \n",
        "    return ' '.join(start_word[1:-1])\n",
        "\n",
        "\n",
        "# Beam search prediction\n",
        "def beam_search_predictions(image_file, beam_index = 3):\n",
        "    start = [word_idx[\"<start>\"]]\n",
        "    \n",
        "    start_word = [[start, 0.0]]\n",
        "    \n",
        "    while len(start_word[0][0]) < max_length:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            now_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "            e = encoding_test[image_file]\n",
        "            preds = final_model.predict([np.array([e]), np.array(now_caps)])\n",
        "            \n",
        "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "            \n",
        "            #Get the top Beam index = 3  predictions and create a \n",
        "            # new list so we can feed them to the model again\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "                    \n",
        "        start_word = temp\n",
        "        # Sorting according to the probabilities\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        # Getting the top words\n",
        "        start_word = start_word[-beam_index:]\n",
        "    \n",
        "    start_word = start_word[-1][0]\n",
        "    intermediate_caption = [idx_word[i] for i in start_word]\n",
        "\n",
        "    final_caption = []\n",
        "    \n",
        "    for i in intermediate_caption:\n",
        "        if i != '<end>':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "    \n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HlXSBEMh315p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_file =\"667626_18933d713e.jpg\"\n",
        "test_image = ''+ image_file\n",
        "Image.open(test_image)\n",
        "\n",
        "print ('Greedy search:', predict_captions(image_file))\n",
        "print ('Beam Search, k=3:', beam_search_predictions(image_file, beam_index=3))\n",
        "print ('Beam Search, k=5:', beam_search_predictions(image_file, beam_index=5))\n",
        "print ('Beam Search, k=7:', beam_search_predictions(image_file, beam_index=7))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}